{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "cbb6ed06-846c-4b1b-9cb7-3052496ef6f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "5763b6ad-47a4-44a9-b8aa-8299e6fbc51f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gaussian(x):\n",
    "    mean = 0\n",
    "    variance = 1\n",
    "    return 1 / (np.sqrt(2 * np.pi * variance)) * np.exp(-((x - mean)**2) / (2 * variance))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "dda16642-b6c6-47e8-9790-58d92ff8960e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PINN(tf.keras.Model):\n",
    "    def __init__(self, layers, regularizer_rate=1e-4):\n",
    "        super(PINN, self).__init__()\n",
    "        self.nn_layers = layers  # Cambié \"layers\" a \"nn_layers\" para evitar conflictos\n",
    "        self.regularizer = tf.keras.regularizers.l2(regularizer_rate)\n",
    "        self.nn_model = self.build_model()\n",
    "\n",
    "    def build_model(self):\n",
    "        model = tf.keras.Sequential()\n",
    "        model.add(tf.keras.layers.InputLayer(input_shape=(1,)))\n",
    "        for width in self.nn_layers[1:-1]:  # Iterar hasta la penúltima capa para agregar BatchNorm antes de la última capa\n",
    "            model.add(tf.keras.layers.Dense(width, activation=tf.nn.tanh,  # Cambiado a función de activación swish\n",
    "                                            kernel_initializer='glorot_normal', \n",
    "                                            kernel_regularizer=self.regularizer))\n",
    "            model.add(tf.keras.layers.BatchNormalization())\n",
    "        model.add(tf.keras.layers.Dense(self.nn_layers[-1], activation=tf.nn.tanh,  # Activación tanh para la última capa\n",
    "                                        kernel_initializer='glorot_normal', \n",
    "                                        kernel_regularizer=self.regularizer))\n",
    "        return model\n",
    "\n",
    "    def call(self, inputs):\n",
    "        return self.nn_model(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "1e2be39d-ed19-43cc-a0ea-45038ff505a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_fn(model, t_inputs, outputs, epsilon, u, DL, K, t_p, Cin):\n",
    "    with tf.GradientTape(persistent=True) as tape:\n",
    "        tape.watch(t_inputs)\n",
    "        predicted = model(t_inputs)\n",
    "        q_bar = K * predicted\n",
    "        dC_dt = tape.gradient(predicted, t_inputs)\n",
    "        dC_dz = tape.gradient(predicted, t_inputs)\n",
    "        dC_dz2 = tape.gradient(dC_dz, t_inputs)\n",
    "\n",
    "    # Balance of non-stationary mass solute equation\n",
    "    eq1 = dC_dt + (1 - epsilon) / epsilon * q_bar + u / epsilon * dC_dz - DL * dC_dz2\n",
    "    loss_eq1 = tf.reduce_mean(tf.square(eq1))\n",
    "    \n",
    "    # Boundary conditions: Danckwerts at the entrance and exit of the column\n",
    "    Cp = 0#tf.where(t_inputs <= t_p, Cin, 0.0)\n",
    "    boundary_loss_1 = tf.square(predicted[0] - Cp + epsilon * DL / u * dC_dz[0])\n",
    "    boundary_loss_2 = tf.square(dC_dz[-1])\n",
    "    \n",
    "    # Combine the boundary losses\n",
    "    loss_boundary = boundary_loss_1 + boundary_loss_2\n",
    "\n",
    "    # Combine losses\n",
    "    combined_loss = loss_eq1*100 + loss_boundary[0]*10\n",
    "\n",
    "    return combined_loss + gaussian(K)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "190faa64-3e24-495f-a642-78d900fd4a26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Loss: 10334.1796875, DL: -49.89999771118164, K: -49.89999771118164\n",
      "Epoch: 1000, Loss: 801.8360595703125, DL: -49.3148193359375, K: -1.644221305847168\n",
      "Epoch: 2000, Loss: 10.399622917175293, DL: -49.3148193359375, K: -0.0015186290256679058\n",
      "Epoch: 3000, Loss: 10.398941993713379, DL: -49.3148193359375, K: -1.045677677780077e-08\n",
      "Epoch: 4000, Loss: 10.398941993713379, DL: -49.3148193359375, K: -2.4429924087000293e-17\n",
      "Epoch: 5000, Loss: 10.398941993713379, DL: -49.3148193359375, K: 3.426195706008711e-34\n",
      "Epoch: 6000, Loss: 10.398941993713379, DL: -49.3148193359375, K: -9.820861558672512e-39\n",
      "Epoch: 7000, Loss: 10.398941993713379, DL: -49.3148193359375, K: -9.820861558672512e-39\n",
      "Epoch: 8000, Loss: 10.398941993713379, DL: -49.3148193359375, K: -9.820861558672512e-39\n",
      "Epoch: 9000, Loss: 10.398941993713379, DL: -49.3148193359375, K: -9.820861558672512e-39\n",
      "Epoch: 10000, Loss: 10.398941993713379, DL: -49.3148193359375, K: -9.820861558672512e-39\n",
      "Epoch: 11000, Loss: 10.398941993713379, DL: -49.3148193359375, K: -9.820861558672512e-39\n",
      "Epoch: 12000, Loss: 10.398941993713379, DL: -49.3148193359375, K: -9.820861558672512e-39\n",
      "Epoch: 13000, Loss: 10.398941993713379, DL: -49.3148193359375, K: -9.820861558672512e-39\n",
      "Epoch: 14000, Loss: 10.398941993713379, DL: -49.3148193359375, K: -9.820861558672512e-39\n"
     ]
    }
   ],
   "source": [
    "# Definición de hiperparámetros y otros valores conocidos\n",
    "layers = [1] +[30] * 5 + [1]  # Aumentar la complejidad del modelo\n",
    "model = PINN(layers)\n",
    "\n",
    "# Decaimiento exponencial de la tasa de aprendizaje\n",
    "lr_schedule = tf.keras.optimizers.schedules.ExponentialDecay(\n",
    "    initial_learning_rate=0.1, \n",
    "    decay_steps=1000, \n",
    "    decay_rate=0.9\n",
    ")\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=lr_schedule)\n",
    "\n",
    "epsilon = 0.37  # Porosidad del lecho\n",
    "u = 5.55  # Velocidad superficial del fluido\n",
    "t_p = 1.0  # Tiempo del pulso de inyección\n",
    "Cin = 523.23  # Concentración del pulso de analito inyectada en la muestra\n",
    "\n",
    "# Inicializar DL y K como variables entrenables\n",
    "DL = tf.Variable(initial_value=-50.0, trainable=True, dtype=tf.float32)\n",
    "K = tf.Variable(initial_value=-50.0, trainable=True, dtype=tf.float32)\n",
    "\n",
    "# Cargar datos de entrenamiento desde el Excel\n",
    "data = pd.read_excel(\"datos_cromatografia.xlsx\", sheet_name=\"training, Cin = 523.23 mg L^-1\")\n",
    "\n",
    "# Normalizar los datos de entrenamiento\n",
    "t_train = tf.convert_to_tensor(data['min'].to_numpy().reshape(-1, 1) / data['min'].max(), dtype=tf.float32)  # tiempo\n",
    "C_train = tf.convert_to_tensor(data['AU'].to_numpy().reshape(-1, 1) / data['AU'].max(), dtype=tf.float32)   # concentración\n",
    "\n",
    "# Entrenamiento\n",
    "epochs = 15000  # Incrementar el número de épocas\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    with tf.GradientTape() as tape:\n",
    "        loss_value = loss_fn(model, t_train, C_train, epsilon, u, DL, K, t_p, Cin)\n",
    "    grads = tape.gradient(loss_value, model.trainable_variables + [DL, K])\n",
    "    optimizer.apply_gradients(zip(grads, model.trainable_variables + [DL, K]))\n",
    "    \n",
    "    if epoch % 1000 == 0:\n",
    "        print(f\"Epoch: {epoch}, Loss: {loss_value.numpy()}, DL: {DL.numpy()}, K: {K.numpy()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a127dfd5-316a-4401-9ea4-ef8c716ffac3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
