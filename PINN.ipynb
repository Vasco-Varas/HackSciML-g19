{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cbb6ed06-846c-4b1b-9cb7-3052496ef6f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-10-02 17:12:58.425131: I tensorflow/core/util/port.cc:111] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2023-10-02 17:12:58.443737: E tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:9342] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2023-10-02 17:12:58.443751: E tensorflow/compiler/xla/stream_executor/cuda/cuda_fft.cc:609] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2023-10-02 17:12:58.443766: E tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:1518] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2023-10-02 17:12:58.447747: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-10-02 17:12:58.814024: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dda16642-b6c6-47e8-9790-58d92ff8960e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PINN(tf.keras.Model):\n",
    "    def __init__(self, layers, regularizer_rate=1e-4):\n",
    "        super(PINN, self).__init__()\n",
    "        self.nn_layers = layers  # Cambié \"layers\" a \"nn_layers\" para evitar conflictos\n",
    "        self.regularizer = tf.keras.regularizers.l2(regularizer_rate)\n",
    "        self.nn_model = self.build_model()\n",
    "\n",
    "    def build_model(self):\n",
    "        model = tf.keras.Sequential()\n",
    "        model.add(tf.keras.layers.InputLayer(input_shape=(1,)))\n",
    "        for width in self.nn_layers[1:-1]:  # Iterar hasta la penúltima capa para agregar BatchNorm antes de la última capa\n",
    "            model.add(tf.keras.layers.Dense(width, activation=tf.nn.swish,  # Cambiado a función de activación swish\n",
    "                                            kernel_initializer='he_normal', \n",
    "                                            kernel_regularizer=self.regularizer))\n",
    "            model.add(tf.keras.layers.BatchNormalization())\n",
    "        model.add(tf.keras.layers.Dense(self.nn_layers[-1], activation=tf.nn.tanh,  # Activación tanh para la última capa\n",
    "                                        kernel_initializer='glorot_normal', \n",
    "                                        kernel_regularizer=self.regularizer))\n",
    "        return model\n",
    "\n",
    "    def call(self, inputs):\n",
    "        return self.nn_model(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1e2be39d-ed19-43cc-a0ea-45038ff505a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_fn(model, t_inputs, outputs, epsilon, u, DL, K, t_p, Cin):\n",
    "    # Usar el modelo para predecir\n",
    "    predicted = model(t_inputs)\n",
    "    \n",
    "    with tf.GradientTape(persistent=True) as tape:\n",
    "        tape.watch(t_inputs)\n",
    "        predicted = model(t_inputs)\n",
    "        q = K * predicted\n",
    "        dC_dt = tape.gradient(predicted, t_inputs)\n",
    "        dC_dz = tape.gradient(dC_dt, t_inputs)\n",
    "        \n",
    "    # Implementar las ecuaciones diferenciales\n",
    "    eq1 = dC_dt + (1 - epsilon) / epsilon * K * dC_dt + u / epsilon * dC_dz - DL * dC_dz\n",
    "    loss_eq1 = tf.reduce_mean(tf.square(eq1))\n",
    "    \n",
    "    # Implementar la condición de borde para t = 0\n",
    "    Cp = tf.where(t_inputs <= t_p, Cin, 0.0)\n",
    "    loss_boundary = tf.reduce_mean(tf.square(predicted - Cp))\n",
    "    \n",
    "    # Agregar término de regularización\n",
    "    loss_reg = tf.abs(DL) + tf.abs(K)\n",
    "    \n",
    "    # Combinar las pérdidas\n",
    "    combined_loss = loss_eq1 + loss_boundary + 0.01 * loss_reg  # El factor 0.01 es arbitrario y puede ser ajustado\n",
    "    \n",
    "    return combined_loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "190faa64-3e24-495f-a642-78d900fd4a26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Calling GradientTape.gradient on a persistent tape inside its context is significantly less efficient than calling it outside the context (it causes the gradient ops to be recorded on the tape, leading to increased CPU and memory usage). Only call GradientTape.gradient inside the context if you actually want to trace the gradient in order to compute higher order derivatives.\n",
      "WARNING:tensorflow:Calling GradientTape.gradient on a persistent tape inside its context is significantly less efficient than calling it outside the context (it causes the gradient ops to be recorded on the tape, leading to increased CPU and memory usage). Only call GradientTape.gradient inside the context if you actually want to trace the gradient in order to compute higher order derivatives.\n",
      "Epoch: 0, Loss: 274182.96875, DL: 0.01100002322345972, K: 0.9989999532699585\n",
      "Epoch: 1000, Loss: 272724.21875, DL: 0.044344935566186905, K: 1.022100567817688\n",
      "Epoch: 2000, Loss: 272724.15625, DL: 0.03475075960159302, K: 1.0076555013656616\n",
      "Epoch: 3000, Loss: 272724.15625, DL: 0.01916314847767353, K: 0.9842893481254578\n",
      "Epoch: 4000, Loss: 272724.15625, DL: -2.889390771088074e-08, K: 0.9486587047576904\n",
      "Epoch: 5000, Loss: 272724.15625, DL: 8.547674951842055e-06, K: 0.8953828811645508\n",
      "Epoch: 6000, Loss: 272724.15625, DL: -5.308341769705294e-06, K: 0.8165578246116638\n",
      "Epoch: 7000, Loss: 272724.15625, DL: -2.2567484847968444e-05, K: 0.7014736533164978\n",
      "Epoch: 8000, Loss: 272724.15625, DL: -2.9378374165389687e-05, K: 0.5381103754043579\n",
      "Epoch: 9000, Loss: 272724.15625, DL: -3.844232196570374e-05, K: 0.3195743262767792\n",
      "Epoch: 10000, Loss: 272724.15625, DL: -4.7303798055509105e-05, K: 0.05494518578052521\n",
      "Epoch: 11000, Loss: 272724.15625, DL: -5.2347360906424e-05, K: 3.0501796572934836e-05\n",
      "Epoch: 12000, Loss: 272724.15625, DL: -5.246590080787428e-05, K: -1.7906145330925938e-06\n",
      "Epoch: 13000, Loss: 272724.15625, DL: -4.952540621161461e-05, K: 5.878041702089831e-05\n",
      "Epoch: 14000, Loss: 272724.15625, DL: -4.548061406239867e-05, K: 5.3187948651611805e-05\n"
     ]
    }
   ],
   "source": [
    "# Definición de hiperparámetros y otros valores conocidos\n",
    "layers = [1, 30, 30, 30, 30, 1]  # Aumentar la complejidad del modelo\n",
    "model = PINN(layers)\n",
    "\n",
    "# Decaimiento exponencial de la tasa de aprendizaje\n",
    "lr_schedule = tf.keras.optimizers.schedules.ExponentialDecay(\n",
    "    initial_learning_rate=0.001, \n",
    "    decay_steps=1000, \n",
    "    decay_rate=0.9\n",
    ")\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=lr_schedule)\n",
    "\n",
    "epsilon = 0.37  # Porosidad del lecho\n",
    "u = 5.55  # Velocidad superficial del fluido\n",
    "t_p = 1.0  # Tiempo del pulso de inyección\n",
    "Cin = 523.23  # Concentración del pulso de analito inyectada en la muestra\n",
    "\n",
    "# Inicializar DL y K como variables entrenables\n",
    "DL = tf.Variable(initial_value=0.01, trainable=True, dtype=tf.float32)\n",
    "K = tf.Variable(initial_value=1.0, trainable=True, dtype=tf.float32)\n",
    "\n",
    "# Cargar datos de entrenamiento desde el Excel\n",
    "data = pd.read_excel(\"datos_cromatografia.xlsx\", sheet_name=\"training, Cin = 523.23 mg L^-1\")\n",
    "\n",
    "# Normalizar los datos de entrenamiento\n",
    "t_train = tf.convert_to_tensor(data['min'].to_numpy().reshape(-1, 1) / data['min'].max(), dtype=tf.float32)  # tiempo\n",
    "C_train = tf.convert_to_tensor(data['AU'].to_numpy().reshape(-1, 1) / data['AU'].max(), dtype=tf.float32)   # concentración\n",
    "\n",
    "# Entrenamiento\n",
    "epochs = 15000  # Incrementar el número de épocas\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    with tf.GradientTape() as tape:\n",
    "        loss_value = loss_fn(model, t_train, C_train, epsilon, u, DL, K, t_p, Cin)\n",
    "    grads = tape.gradient(loss_value, model.trainable_variables + [DL, K])\n",
    "    optimizer.apply_gradients(zip(grads, model.trainable_variables + [DL, K]))\n",
    "    \n",
    "    if epoch % 1000 == 0:\n",
    "        print(f\"Epoch: {epoch}, Loss: {loss_value.numpy()}, DL: {DL.numpy()}, K: {K.numpy()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a127dfd5-316a-4401-9ea4-ef8c716ffac3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
