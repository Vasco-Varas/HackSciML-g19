{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "513da0f1-4e91-4b60-a9d3-d02ba7b06dc9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-10-02 21:25:00.362333: I tensorflow/core/util/port.cc:111] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2023-10-02 21:25:00.381017: E tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:9342] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2023-10-02 21:25:00.381035: E tensorflow/compiler/xla/stream_executor/cuda/cuda_fft.cc:609] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2023-10-02 21:25:00.381046: E tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:1518] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2023-10-02 21:25:00.384923: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-10-02 21:25:00.718073: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "Using backend: tensorflow.compat.v1\n",
      "Other supported backends: tensorflow, pytorch, jax, paddle.\n",
      "paddle supports more examples now and is recommended.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/vasco/dev/Python-Projects/HackSciML-g19/venv/lib/python3.10/site-packages/tensorflow/python/compat/v2_compat.py:108: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "non-resource variables are not supported in the long term\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-10-02 21:25:01.084789: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:894] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-10-02 21:25:01.098417: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:894] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-10-02 21:25:01.098498: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:894] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enable just-in-time compilation with XLA.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import deepxde as dde"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0471ec52-5cb1-4493-89be-725c484fb346",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the PDE\n",
    "def pde(data):\n",
    "    t, z, C, q_bar = data[:, 0:1], data[:, 1:2], data[:, 2:3], data[:, 3:4]\n",
    "\n",
    "    epsilon = 0.37\n",
    "    u = 5.55\n",
    "\n",
    "    dC_dt = dde.grad.jacobian(C, t)\n",
    "    dC_dz = dde.grad.jacobian(C, z)\n",
    "    d2C_dz2 = dde.grad.hessian(C, z)\n",
    "    dq_bar_dt = dde.grad.jacobian(q_bar, t)\n",
    "\n",
    "    # Assuming DL and K are trainable parameters, we can treat them as variables initially\n",
    "    DL = dde.Variable(1.0, trainable=True, constraint=lambda x: x > 0)\n",
    "    K = dde.Variable(1.0, trainable=True, constraint=lambda x: x > 0)\n",
    "\n",
    "    eq1 = dC_dt + (1 - epsilon) / epsilon * dq_bar_dt + u / epsilon * dC_dz - DL * d2C_dz2\n",
    "    eq2 = q_bar - K * C\n",
    "\n",
    "    return [eq1, eq2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "da4778ad-6751-4e41-a7b2-d62c4774c41f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the computational domain\n",
    "geom = dde.geometry.Interval(0, 15)  # z domain\n",
    "timedomain = dde.geometry.TimeDomain(0, 1)  # t domain\n",
    "domain = dde.geometry.GeometryXTime(geom, timedomain)\n",
    "\n",
    "# Define Cp based on given conditions\n",
    "def Cp(t, C_in, t_p):\n",
    "    return np.where(t <= t_p, C_in, 0)\n",
    "\n",
    "# Initial condition\n",
    "def func_ic(x):\n",
    "    return np.zeros_like(x[:, 0:1])\n",
    "\n",
    "ic = dde.IC(domain, func_ic, lambda _, on_initial: on_initial)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5e6816a1-80b5-4bbb-af41-ef2f89d49e5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "bcs = []  # If there are no boundary conditions, just use an empty list\n",
    "\n",
    "# Define neural network and solve using DeepXDE\n",
    "data = dde.data.TimePDE(\n",
    "    domain,\n",
    "    pde,\n",
    "    bcs,\n",
    "    num_domain=4000,\n",
    "    num_boundary=200,\n",
    "    num_initial=100,\n",
    "    train_distribution=\"pseudo\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "be3fbf2f-e527-4beb-ae27-3d0c56aca191",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Compiling model...\n",
      "Building feed-forward neural network...\n",
      "'build' took 0.027207 s\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/vasco/dev/Python-Projects/HackSciML-g19/venv/lib/python3.10/site-packages/deepxde/nn/tensorflow_compat_v1/fnn.py:116: UserWarning: `tf.layers.dense` is deprecated and will be removed in a future version. Please use `tf.keras.layers.Dense` instead.\n",
      "  return tf.layers.dense(\n",
      "2023-10-02 21:25:55.463288: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:894] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-10-02 21:25:55.463395: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:894] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-10-02 21:25:55.463435: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:894] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-10-02 21:25:55.733039: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:894] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-10-02 21:25:55.733129: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:894] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-10-02 21:25:55.733164: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:47] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.\n",
      "2023-10-02 21:25:55.733178: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:894] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-10-02 21:25:55.733236: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1886] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 183 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 4090, pci bus id: 0000:01:00.0, compute capability: 8.9\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "No gradients provided for any variable, check your graph for ops that do not support gradients, between variables [\"<tf.Variable 'dense/kernel:0' shape=(2, 20) dtype=float32_ref>\", \"<tf.Variable 'dense/bias:0' shape=(20,) dtype=float32_ref>\", \"<tf.Variable 'dense_1/kernel:0' shape=(20, 20) dtype=float32_ref>\", \"<tf.Variable 'dense_1/bias:0' shape=(20,) dtype=float32_ref>\", \"<tf.Variable 'dense_2/kernel:0' shape=(20, 20) dtype=float32_ref>\", \"<tf.Variable 'dense_2/bias:0' shape=(20,) dtype=float32_ref>\", \"<tf.Variable 'dense_3/kernel:0' shape=(20, 2) dtype=float32_ref>\", \"<tf.Variable 'dense_3/bias:0' shape=(2,) dtype=float32_ref>\"] and loss Tensor(\"Sum:0\", shape=(), dtype=float32).",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 6\u001b[0m\n\u001b[1;32m      2\u001b[0m net \u001b[38;5;241m=\u001b[39m dde\u001b[38;5;241m.\u001b[39mmaps\u001b[38;5;241m.\u001b[39mFNN(layer_size, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtanh\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGlorot normal\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      4\u001b[0m model \u001b[38;5;241m=\u001b[39m dde\u001b[38;5;241m.\u001b[39mModel(data, net)\n\u001b[0;32m----> 6\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompile\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43madam\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1e-3\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmetrics\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43ml2 relative error\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      8\u001b[0m losshistory, train_state \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mtrain(epochs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10000\u001b[39m)\n",
      "File \u001b[0;32m~/dev/Python-Projects/HackSciML-g19/venv/lib/python3.10/site-packages/deepxde/utils/internal.py:22\u001b[0m, in \u001b[0;36mtiming.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[38;5;129m@wraps\u001b[39m(f)\n\u001b[1;32m     20\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrapper\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m     21\u001b[0m     ts \u001b[38;5;241m=\u001b[39m timeit\u001b[38;5;241m.\u001b[39mdefault_timer()\n\u001b[0;32m---> 22\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     23\u001b[0m     te \u001b[38;5;241m=\u001b[39m timeit\u001b[38;5;241m.\u001b[39mdefault_timer()\n\u001b[1;32m     24\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m config\u001b[38;5;241m.\u001b[39mrank \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[0;32m~/dev/Python-Projects/HackSciML-g19/venv/lib/python3.10/site-packages/deepxde/model.py:136\u001b[0m, in \u001b[0;36mModel.compile\u001b[0;34m(self, optimizer, lr, loss, metrics, decay, loss_weights, external_trainable_variables)\u001b[0m\n\u001b[1;32m    133\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexternal_trainable_variables \u001b[38;5;241m=\u001b[39m external_trainable_variables\n\u001b[1;32m    135\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m backend_name \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtensorflow.compat.v1\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m--> 136\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_compile_tensorflow_compat_v1\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mloss_fn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdecay\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mloss_weights\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    137\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m backend_name \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtensorflow\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    138\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compile_tensorflow(lr, loss_fn, decay, loss_weights)\n",
      "File \u001b[0;32m~/dev/Python-Projects/HackSciML-g19/venv/lib/python3.10/site-packages/deepxde/model.py:193\u001b[0m, in \u001b[0;36mModel._compile_tensorflow_compat_v1\u001b[0;34m(self, lr, loss_fn, decay, loss_weights)\u001b[0m\n\u001b[1;32m    191\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutputs_losses_train \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnet\u001b[38;5;241m.\u001b[39moutputs, losses_train]\n\u001b[1;32m    192\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutputs_losses_test \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnet\u001b[38;5;241m.\u001b[39moutputs, losses_test]\n\u001b[0;32m--> 193\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrain_step \u001b[38;5;241m=\u001b[39m \u001b[43moptimizers\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    194\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtotal_loss\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mopt_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlearning_rate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdecay\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdecay\u001b[49m\n\u001b[1;32m    195\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/dev/Python-Projects/HackSciML-g19/venv/lib/python3.10/site-packages/deepxde/optimizers/tensorflow_compat_v1/optimizers.py:72\u001b[0m, in \u001b[0;36mget\u001b[0;34m(loss, optimizer, learning_rate, decay)\u001b[0m\n\u001b[1;32m     62\u001b[0m     optim \u001b[38;5;241m=\u001b[39m hvd\u001b[38;5;241m.\u001b[39mDistributedOptimizer(\n\u001b[1;32m     63\u001b[0m         optim,\n\u001b[1;32m     64\u001b[0m         compression\u001b[38;5;241m=\u001b[39mhvd_opt_options[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcompression\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     69\u001b[0m         ],\n\u001b[1;32m     70\u001b[0m     )\n\u001b[1;32m     71\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m tf\u001b[38;5;241m.\u001b[39mcontrol_dependencies(update_ops):\n\u001b[0;32m---> 72\u001b[0m     train_op \u001b[38;5;241m=\u001b[39m \u001b[43moptim\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mminimize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mloss\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mglobal_step\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mglobal_step\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     73\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m train_op\n",
      "File \u001b[0;32m~/dev/Python-Projects/HackSciML-g19/venv/lib/python3.10/site-packages/tensorflow/python/training/optimizer.py:481\u001b[0m, in \u001b[0;36mOptimizer.minimize\u001b[0;34m(self, loss, global_step, var_list, gate_gradients, aggregation_method, colocate_gradients_with_ops, name, grad_loss)\u001b[0m\n\u001b[1;32m    479\u001b[0m vars_with_grad \u001b[38;5;241m=\u001b[39m [v \u001b[38;5;28;01mfor\u001b[39;00m g, v \u001b[38;5;129;01min\u001b[39;00m grads_and_vars \u001b[38;5;28;01mif\u001b[39;00m g \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m]\n\u001b[1;32m    480\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m vars_with_grad:\n\u001b[0;32m--> 481\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    482\u001b[0m       \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo gradients provided for any variable, check your graph for ops\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    483\u001b[0m       \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m that do not support gradients, between variables \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m and loss \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m\n\u001b[1;32m    484\u001b[0m       ([\u001b[38;5;28mstr\u001b[39m(v) \u001b[38;5;28;01mfor\u001b[39;00m _, v \u001b[38;5;129;01min\u001b[39;00m grads_and_vars], loss))\n\u001b[1;32m    486\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mapply_gradients(grads_and_vars, global_step\u001b[38;5;241m=\u001b[39mglobal_step,\n\u001b[1;32m    487\u001b[0m                             name\u001b[38;5;241m=\u001b[39mname)\n",
      "\u001b[0;31mValueError\u001b[0m: No gradients provided for any variable, check your graph for ops that do not support gradients, between variables [\"<tf.Variable 'dense/kernel:0' shape=(2, 20) dtype=float32_ref>\", \"<tf.Variable 'dense/bias:0' shape=(20,) dtype=float32_ref>\", \"<tf.Variable 'dense_1/kernel:0' shape=(20, 20) dtype=float32_ref>\", \"<tf.Variable 'dense_1/bias:0' shape=(20,) dtype=float32_ref>\", \"<tf.Variable 'dense_2/kernel:0' shape=(20, 20) dtype=float32_ref>\", \"<tf.Variable 'dense_2/bias:0' shape=(20,) dtype=float32_ref>\", \"<tf.Variable 'dense_3/kernel:0' shape=(20, 2) dtype=float32_ref>\", \"<tf.Variable 'dense_3/bias:0' shape=(2,) dtype=float32_ref>\"] and loss Tensor(\"Sum:0\", shape=(), dtype=float32)."
     ]
    }
   ],
   "source": [
    "layer_size = [2] + [20] * 3 + [2]\n",
    "net = dde.maps.FNN(layer_size, \"tanh\", \"Glorot normal\")\n",
    "\n",
    "model = dde.Model(data, net)\n",
    "\n",
    "model.compile(\"adam\", lr=1e-3, metrics=[\"l2 relative error\"])\n",
    "\n",
    "losshistory, train_state = model.train(epochs=10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "629c77d2-0da2-4887-b224-5c401d6d8548",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
